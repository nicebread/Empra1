---
title: Session 13 - Data Analysis
number-sections: false
description: We do a first round of data analysis
---

# Overview

| Topic                                   | Duration | Notes |
| :-------------------------------------- | :------: | ----- |
| Feedback participant acquisition        |    20    |       |
| Preparation of data analysis            |    45    |       |
| Power analysis (finalized)              |    30    |       |
| Doing the confimatory analysis together |    60    |       |
| Amending the preregistration            |    40    |       |

: {.striped}


## Preparation

1. Create a new folder called "Empra-Studie" and add all default subfolders as explained in the "[Good Coding Practices](../slides/Good_coding_practices/Good_coding_practices.qmd#subdirectory-organization)" slides.
2. Create an empty *apaquarto* document in the top level folder. Please consult the [apaquarto documentation](https://wjschne.github.io/apaquarto/writing.html), or our [tutorial](https://lmu-osc.github.io/introduction-to-Quarto/apa7_manuscript.html).
3. Add your meta-data to the YAML header; most importantly your author information (because without at least one author, the PDF will not be compiled). Add your ORCID id that you created in the first homework.
4. Add all default section headings of an APA paper. "Methods", "Results", "Discussion" should be Level 1 (with `#`), and so on.
5. Place the primary data files in the `raw_data` subfolder.
6. Create a new R script and name it `01_Preprocessing.R`. Add your meta-information on top of the script (see screenshot in [Good Coding Practices](../slides/Good_coding_practices/Good_coding_practices.qmd#code-commenting-1) as an example). Load both data files and inspect them with the `summarytools` package:

```r
# `dat` should be the name of your data frame
library(summarytools)
view(dfSummary(dat))
```



## The statistical model

In an experimental pre-post treatment-control-group design, we compare the means of two groups (control vs. treatment) and at the same time control for the baseline values before the treatment. For a proper analysis, we need mixed effects models (either a repeated measures ANOVA with one within- and one between-factor, or a linear mixed model). You learn the latter in the MSc program. Here, we implement a simpler model that you can master with your current knowledge. Importantly, that simpler model is *statistically valid* (albeit, not optimal).

### The basic model

The basic analysis only focuses on the randomized group membership (control vs. treatment), and ignores the pre-treatment scores. This is a valid analysis, but has a smaller statistical power than the improved model. The basic model can be done with a simple paired *t*-test, comparing the outcome variable (*post*-treatment intimacy motivation) between the both experimental conditions (which are paired, as all participants had all conditions).

This model answers the question: Is the intimacy motivation after our treatment higher, compared to the control condition? And if the condition was randomly assigned, it can be rephrased as: Does the treatment causally affect the intimacy motivation?

This model (comparing the post-treatment outcome values) is an unbiased and therefore valid estimate of the causal effect of the treatment. "Unbiased" means: Across many (hypothetical) experiments with this design, the mean of all estimated treatment effects is not systematically biased in either direction. Put in other words: The estimate of each single experiment can be off, but on average, they are correct. As all participants went through both conditions, we control for all pre-existing differences between persons. This already gives us a much higher power than a between-person design.

### Dealing with three groups (up, down, neutral) & Multiple Testing

We do multiple pairwise comparisons (up vs. neutral and down vs. neutral) for all six intervention categories, amounting to 2*6 = 12 hypothesis tests. We need to correct for multiple testing. We use a Bonferroni correction, which is the most conservative correction method. It is simple to apply: We divide the alpha level by the number of tests. In our case, we have 12 tests, so we we need a $\alpha$-level of .05/12 = .0042 to reject each of the null hypotheses.


## Power Analysis (finalized)

```r
## install/load necessary libraries
library(pwr) # for power analyses

# reported values from Mashek et al. (2011)
M_diff <- 6.02 - 5.69
SD1 <- 1.34
SD2 <- 1.23
# n_overall = 91; assuming equal sample sizes (not reported in paper)
n1 <- 45
n2 <- 46
SD_pooled <- sqrt(((n1-1)*SD1^2 + (n2-1)*SD2^2) / (n1+n2-2))

d_between <- M_diff / SD_pooled
d_between

# We can convert the between-person effect size to a within-person effect size by dividing it by the square root of 2*(1-r), where r is the correlation between the control and the treatment values. From pilot data, we assume this correlation to be r = .8.
r <- 0.8

# formula: see, for example, Lakens (2013)
d_within <- d_between / sqrt(2*(1-r))
d_within

# If we do a pure post-treatment comparison, we would need:
# (one-sided testing, "loose" alpha level of .05; with a Bonferroni correction for 12 hypothesis tests)
# This requires 77 participants
pwr.t.test(d=d_within, power = .80, sig.level = .05 / 12, alternative="greater", type="paired")


# Furthermore, every participant only does 3 out 6 interventions, so we need double the size.
# Finally, we expect a dropout of 20%, so we need to adjust the sample size accordingly
77*2/.8

# --> we would need 193 participants that start the study.
```

**Where to store this power analysis script?**
The script belongs to the preregistration, so I suggest to upload it as supplemental material together with the prereg.
In addition, you should store it within your project folder. As the script is independent of the actual data analysis, I would store it in the `/doc` subfolder. 


### How to report the power analysis in the preregistration (and in the Methods section)?

Here's a possible description of the power analysis for the preregistration and the methods section:

> We computed a basic post-treatment comparison of two groups, which is equivalent to a paired *t*-test. We use the effect size reported in Mashek et al. (2011) as expected effect size, because this is the published study closest to our own design. We use the descriptive values reported in Table 5 ("Postmanipulation variables: Desired closeness") to compute Cohen's *d* for a between-person comparison ($d_{\text{between}}$ = 0.26). We then convert this between-person effect size to a within-person effect size. Based on some pilot data, we assume a correlation of .8 between the control and the treatment values, which yields a $d_{\text{within}}$ = 0.41. As we test multiple interventions in a screening scenario, we use a "loose" $\alpha$ level of .05, and do a Bonferroni correction for 12 hypothesis tests, which leads to a corrected $\alpha$ level of .05/12=.0042. Setting the desired power to 80%, the required sample size for a paired *t*-test is *n* = 77. However, as our participants only receive 3 random interventions out of 6, we need to recruit the double number of participants, which leads to a required sample size of *n* = 154. Furthermore, we expect a dropout rate of 20%, which leads to a final required sample size of *n* = 193 to enroll into our study.

 
**Group Task**: Compare this draft for the preregistration / method section with the best practices you learned in the lecture on [Reporting of Power Analyses](../slides/Power-Analysis Reporting/How-to-report-a-power-analysis.qmd). At least one piece of information is missing here! Can you find it?

::: {.callout-tip title="Hint" collapse="true"}
It was not mentioned that it was one-sided. And it is not mentioned that it is an *a-priori* power analysis; but in the context of a preregistration this is inplicitly clear.
:::

