---
title: "Power Analysis: Introduction"
subtitle: "Slides: https://osf.io/t5rjf/"
author:
  - name: Felix Schönbrodt
    orcid: 0000-0002-8282-3910
    email: felix.schoenbrodt@psy.lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: 2024-04-19
footer: "Forschungsorientierte Praktikum I – Empirisches Praktikum, Ludwig-Maximilians-Universität München"
format: 
  FOMO-revealjs: 
    output-ext: slide.html
    transition: slide    
  html: default
revealjs-plugins:
  - attribution 
---
# Part I: General concepts of power analysis

# What is statistical power?

## A 2x2 classification matrix {visibility="hidden"}

|                                  |Reality: Effect present | Reality: No effect present | 
|----------------------------------|:-----------------------|:---------------------------|
| **Test indicates: Effect present**  | True Positive          | False Positive             |
| **Test indicates: No effect present**| False Negative         | True Negative              |

## A 2x2 classification matrix
![](img/matrix1.png)

##
![](img/meme1.png)

::: {.footer}
[https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg](https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg)
::: 

##
![](img/meme2.png)

::: {.footer}
[https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg](https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg)
::: 

##
![](img/matrix2.png)

## How to do a power analysis {visibility="hidden"}

::: {.r-stretch}

```{mermaid}

flowchart LR
    E["` **Effect Size** 
    (see Part II of the workshop)`"]
    D["` **Desired Power**
     usually 80%, 90% recommended for critical
studies (Bondavera, 2013)`"]
    L["` **Significance Level**
    0.05? 0.005 (Benjamin et al., 2018)? justify
your alpha (Lakens et al., 2018)? `"]
    S[Sample Size]
    L --> S
    D --> S
    E --> S


```

:::

::: {.smaller}
Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E. J., Berk, R., ... & Cesarini, D. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6. doi:10.1038/s41562-017-0189-z
Bondareva, D. (2013). Introduction to Power Analysis. Presentation for EPSE 482 Introduction to Statistics for Research in Education. Slides available at [slideshare.net/dbondareva/introduction-to-power-analysis](slideshare.net/dbondareva/introduction-to-power-analysis)
Lakens, D., Adolfi, F. G., Albers, C. J., Anvari, F., Apps, M. A., Argamon, S. E., ... & Buchanan, E. M. (2018). Justify your alpha. Nature Human Behaviour, 2, 168-171. doi: 10.1038/s41562-018-0311-x
:::

## How to do a power analysis 

::: {.r-stretch}

```{mermaid}

flowchart LR
    E[" Effect Size 
    (see Part II of the workshop)"]
    D[" Desired Power
     usually 80%, 90% recommended for critical
studies (Bondavera, 2013)"]
    L[" Significance Level
    0.05? 0.005 (Benjamin et al., 2018)? justify
your alpha (Lakens et al., 2018)? "]
    S[Sample Size]
    L --> S
    D --> S
    E --> S


```

:::

::: {.footer}
Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E. J., Berk, R., ... & Cesarini, D. (2018). Redefine statistical
significance. Nature Human Behaviour, 2(1), 6. doi:10.1038/s41562-017-0189-z

Bondareva, D. (2013). Introduction to Power Analysis. Presentation for EPSE 482 Introduction to Statistics for Research in Education. Slides available at
[slideshare.net/dbondareva/introduction-to-power-analysis](slideshare.net/dbondareva/introduction-to-power-analysis)

Lakens, D., Adolfi, F. G., Albers, C. J., Anvari, F., Apps, M. A., Argamon, S. E., ... & Buchanan, E. M. (2018). Justify your alpha. Nature Human Behaviour,
2, 168-171. doi: 10.1038/s41562-018-0311-x
:::

## Power is a frequentist property - beware of fallacies!

### Power is a pre-data measure (i.e., before data are collected) that averages over infinite hypothetical experiments

- Only one of these hypothetical experiments will actually be observed
- **Power is a property of the test procedure/ the design – not of a single study’s outcome!**

### Power is conditional on a hypothetical effect size – not conditional on the actual data obtained

- “Once the actual data are available, a power calculation is no longer conditioned on what is known, no longer corresponds to a valid inference, and may now be misleading.” ➙ for inference better use likelihood ratios or Bayes factors. Then pre-data power considerations are irrelevant.

::: {.footer}
Wagenmakers, E.-J., Verhagen, J., Ly, A., Bakker, M., Lee, M. D., Matzke, D., Rouder, J. N., et al. (2014). A power fallacy.Behavior Research Methods, 47, 913–917. doi:10.3758/s13428-014-0517-4
::: 


## Post hoc power considerations

- Using the observed effect size to calculate „post hoc
power“ is meaningless (it‘s just a transformation of the p-
value)
- It is however meaningful to estimate the power you have
achieved with your collected sample size and the a priori
assumed effect size („sensitivity power analysis“)

::: {.footer}
[https://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html](https://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html)
Gelman, A. (2019). Don't calculate post-hoc power using observed estimate of effect size. Ann. Surg, 269, e9-e10
::: 

# Why power is important

## Exercise: <br> Given that p < .05: <br> What is the probability that a real effect exists in the population ➙ prob(H₁|D)

## {visibility="hidden"}

this part is not finished as I was unable to find a way to include the text outside the box while not putting another box around it -> see code below

::: {.r-stretch}

```{mermaid}
flowchart TB
    c1-->a2
    subgraph one
    a2
    end
    subgraph two
    b1-->b2
    end
    subgraph three
    c1-->c2
    end

```
:::

##
![](img/graph1.png)

::: {.footer}
Nuzzo, R. (2014). Statistical errors. Nature. Colquhoun, D. (2014). An investigation of the false discovery rate and the misinterpretation of p-values. Royal Society Open Science, 1(3), 140216–140216. [http://doi.org/10.1073/pnas.1313476110](http://doi.org/10.1073/pnas.1313476110)
::: 


##
![](img/graph2.png)

::: {.footer}
Nuzzo, R. (2014). Statistical errors. Nature. Colquhoun, D. (2014). An investigation of the false discovery rate and the misinterpretation of p-values. Royal Society Open Science, 1(3), 140216–140216. [http://doi.org/10.1073/pnas.1313476110](http://doi.org/10.1073/pnas.1313476110)
::: 


##
![](img/graph3.png)


::: {.footer}
Nuzzo, R. (2014). Statistical errors. Nature. Colquhoun, D. (2014). An investigation of the false discovery rate and the misinterpretation of p-values. Royal Society Open Science, 1(3), 140216–140216. [http://doi.org/10.1073/pnas.1313476110](http://doi.org/10.1073/pnas.1313476110)
::: 


##
![](img/article1.png)

Assumed that our tested hypothesis are true in 30% of all cases (which is a not too risky research scenario):

- **A typical neuroscience study must "fail" (p > α) in 90% of all cases**

- **In the most likely outcome of p > .05, we have no idea whether a) the effect does not exist, or b) we simply missed the effect. Virtually no knowledge has been gained.**

::: {.footer}
Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: why small sample size  undermines the reliability of neuroscience. Nat Rev Neurosci, 14(5), 365–376. doi:10.1038/nrn3475
::: 

## {visibility="hidden"}

![](img/graph4.jpg) 

::: {.footer}
Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. arXiv:1605.09511 [physics, stat]. Retrieved from [http://arxiv.org/abs/1605.09511](http://arxiv.org/abs/1605.09511)
::: 


##

![](img/text1.png)

::: {.footer}
Schönbrodt, F. D. & Wagenmakers, E.-J. (2016). Bayes Factor Design Analysis: Planning for Compelling Evidence. [http://dx.doi.org/10.2139/ssrn.2722435](http://dx.doi.org/10.2139/ssrn.2722435) 
::: 

## A power analysis helps you to find a balance between...

![](img/balance.png)

::: {.footer}
Pictures from [nature.com/articles/srep43627](nature.com/articles/srep43627), [pngimg.com/download/23544](pngimg.com/download/23544), [whyopenresearch.org/funding](whyopenresearch.org/funding)
::: 

# Researcher‘s intuitions about power

## Researcher’s intuitions about power

![](img/intuition.png)

::: {.footer}
Bakker, M., Hartgerink, C. H. J., Wicherts, J. M., & van der Maas, H. L. J. (2016). Researchers’ Intuitions About Power in Psychological Research. Psychological Science, 27(8), 1069–1077. [http://doi.org/10.1177/0956797616647519](http://doi.org/10.1177/0956797616647519)
::: 

## Calibrate your power feeling

![](img/table1.png)

## Calibrate your power feeling
![](img/table2.png)

# Clever designs go a long way

## The power of within-SS designs

![](img/graph5.jpg){fig-align="left"}

- Why? Each person is his/her own control group
- For example, for the paired t-test:


  - By computing the within-person difference scores, all between-person variance (which contributes to error variance), gets removed
  - Less error variance &rarr;  less noise &rarr; (relatively) more signal &rarr; larger effect size


## Increase power with reliable measures

![](img/table3.png)

- Cohen’s d = 0.4
- N = 30
- pre-post-test

::: {.footer}
Heo, M., Kim, N., & Faith, M. S. (2015). Statistical power as a function of Cronbach alpha of instrument questionnaire items. 
BMC Medical Research Methodology, 15. doi:10.1186/s12874-015-0070-6
::: 


## Specific predictions? <br> Use one-tailed tests!

- One-tailed tests have a higher power than two-tailed tests
- Particularly recommended in combination with a preregistration
- Most power analysis approaches (G*Power, R packages) allow you to chose between one- and two-tailed tests


# Any questions so far?


# Part II: <br> Effect sizes / smallest effects of interests

# Common effect size metrics

##  Common effect sizes

![](img/table4.png)

##  Effect size transformations
![](img/table5.png)

##  Effect size transformations
![](img/table6.png)

::: {.footer}
Borenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Effect sizes based on correlations. In Introduction to Meta-Analysis, p. 45-49.
Brysbaert, M. (2019) How Many Participants Do We Have to Include in Properly Powered Experiments? A Tutorial of Power Analysis with Reference
Tables. Journal of Cognition, 2(1): 16, pp. 1–38. DOI: [https://doi.org/10.5334/joc.72](https://doi.org/10.5334/joc.72)
Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and
ANOVAs. Frontiers in Psychology, 4. [https://doi.org/10.3389/fpsyg.2013.00863](https://doi.org/10.3389/fpsyg.2013.00863)
::: 

## Converting among effect sizes
![](img/conversion1.png)

::: {.footer}
Borenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Converting among effect sizes. In Introduction to Meta-Analysis, p. 45-49.
Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and
ANOVAs. Frontiers in Psychology, 4. [https://doi.org/10.3389/fpsyg.2013.00863](https://doi.org/10.3389/fpsyg.2013.00863)
::: 

## Converting among effect sizes
![](img/conversion2.png)

::: {.footer}
[https://www.ibm.com/support/pages/effect-size-relationship-between-partial-eta-squared-cohens-f-and-cohens-d](https://www.ibm.com/support/pages/effect-size-relationship-between-partial-eta-squared-cohens-f-and-cohens-d)
Brysbaert, M. (2019) How Many Participants Do We Have to Include in Properly Powered Experiments? A Tutorial of Power Analysis with Reference
Tables. Journal of Cognition, 2(1): 16, pp. 1–38. DOI: [https://doi.org/10.5334/joc.72](https://doi.org/10.5334/joc.72)
::: 

## Converting among effect sizes
![](img/conversion3.png)

::: {.footer}
Brysbaert, M. (2019) How Many Participants Do We Have to Include in Properly Powered Experiments? A Tutorial of Power Analysis with Reference
Tables. Journal of Cognition, 2(1): 16, pp. 1–38. DOI: [https://doi.org/10.5334/joc.72](https://doi.org/10.5334/joc.72)
::: 

## Converting among effect sizes
![](img/conversion4.png)




# Getting a feeling about effect sizes

## How do these effect sizes look like?
![](img/graph6.png){fig-align="center"}

::: {.footer}
[https://rpsychologist.com/d3/cohend/](https://rpsychologist.com/d3/cohend/)
::: 

## How do these effect sizes look like?
![](img/graph7.png){fig-align="center"}

::: {.footer}
[https://rpsychologist.com/d3/cohend/](https://rpsychologist.com/d3/cohend/)
::: 

## How do these effect sizes look like?
![](img/graph8.png){fig-align="center"}

::: {.footer}
[https://rpsychologist.com/d3/cohend/](https://rpsychologist.com/d3/cohend/)
::: 


## Guess the correlation
![](img/corrguess1.png){fig-align="center"}

::: {.footer}
[http://guessthecorrelation.com/](http://guessthecorrelation.com/)
::: 

## Guess the correlation
![](img/corrguess2.png){fig-align="center"}

::: {.footer}
[http://guessthecorrelation.com/](http://guessthecorrelation.com/)
::: 

## Understanding effect sizes

![](img/effectsize.png){fig-align="left"}

More understandable metrics: „Common Language Effect Size“, CLES:

- …the probability that a randomly sampled person from one group will have a higher observed measurement than a randomly sampled person from the other group (for between designs)
- …or (for within-designs) the probability that an individual has a higher value on one measurement than the other.

::: {.footer}
McGraw, K. O., & Wong, S. P. (1992). A common language effect size statistic. Psychological Bulletin, 111(2), 361–365. [https://doi.org/10.1037/0033-2909.111.2.361](https://doi.org/10.1037/0033-2909.111.2.361)
Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4. [https://doi.org/10.3389/fpsyg.2013.00863](https://doi.org/10.3389/fpsyg.2013.00863)
::: 

## Understanding effect sizes

Example: d = 0.4, n=55 in each group

- Repeated-measures factor: 61% of the participants change into the expected direction
- Between-groups factor: 61% chance of finding the expected ordering if you test a random participant from each sample

# Typical effect sizes

## Cohen‘s conventions
![](img/cohen.png){fig-align="left"}

Is this reasonable?

## Typical reported effect sizes I

e.g., Richard, Bond, & Stokes-Zoota (2003):

- Meta-meta-analysis; > 25.000 studies, > 8.000.000
participants
- mean effect r = .21 (across literature SD = .15); median = .18

![](img/reportedeffectsize.png){fig-align="center"}

::: {.footer}
Richard, F. D., Bond, C. F. J., & Stokes-Zoota, J. J. (2003). One Hundred Years of Social Psychology Quantitatively Described. Review of General Psychology, 7(4), 331–363. doi:10.1037/1089-2680.7.4.331
:::

## Typical reported effect sizes I
e.g., Richard, Bond, & Stokes-Zoota (2003):
![](img/table7.png){fig-align="center"}

## Typical reported effect sizes II
e.g., Bosco et al. (2015):

- 147,328 correlations from Journal of Applied Psychology and Personnel Psychology
- median effect: r = .16,  mean effect r = .22 (SD = .20)

![](img/graph9.jpg){fig-align="center"}



::: {.footer}
Bosco, F. A., Aguinis, H., Singh, K., Field, J. G., & Pierce, C. A. (2015). Correlational effect size benchmarks. Journal of Applied Psychology, 100(2), 431–449. [http://doi.org/10.1037/a0038047](http://doi.org/10.1037/a0038047)
:::

## Typical reported effect sizes III
e.g., Hill et al. (2008):

- How does the effect of an intervention compare to a typical year of growth in
school?

![](img/hedgesg.png){fig-align="center"}

::: {.footer}
Hill, C. J., Bloom, H. S., Black, A. R., & Lipsey, M. W. (2008). Empirical Benchmarks for Interpreting Effect Sizes in Research. Child Development Perspectives, 2, 172–177. doi:10.1111/j.1750-8606.2008.00061.x
:::

## Typical reported effect sizes IV
e.g., Funder & Ozer (2019):

![](img/graph10.jpg){fig-align="center"}

::: {.footer}
Funder, D. C., & Ozer, D. J. (2019). Evaluating Effect Size in Psychological Research: Sense and Nonsense. Advances in Methods and Practices in Psychological Science, 2, 156–168. doi:10.1177/2515245919847202
:::

## Typical reported effect sizes V
e.g., Aguinis, Beaty, Boik, & Pierce (2005):
- Effect size of interaction from dichotomous moderator and continuous predictor
![](img/table8.png){fig-align="center"}

::: {.footer}
Aguinis, H., Beaty, J. C., Boik, R. J., & Pierce, C. A. (2005). Effect size and power in assessing moderating effects of categorical variables using multiple regression: a 30-year review. Journal of applied psychology, 90(1), 94.
:::

## Other benchmarks I
Average placebo effect? <br> d = 0.24 [0.17; 0.31]!

::: {.footer}
Hróbjartsson, A., & Gøtzsche, P. C. (2004). Is the placebo powerless? Update of a systematic review with 52 new randomized trials comparing placebo with no treatment. Journal of internal medicine, 256(2), 91-100.
:::

## Other benchmarks II (ES: d)

![](img/graph11.png){fig-align="center"}

::: {.footer}
Luhmann, M., Hofmann, W., Eid, M., & Lucas, R. E. (2012). Subjective well-being and adaptation to life events: a meta-analysis. Journal of Personality and
Social Psychology, 102(3), 592–615. [http://doi.org/10.1037/a0025948](http://doi.org/10.1037/a0025948)
:::

# The trustworthiness of effect sizes in the literature

##

![](img/sampletopower.png){fig-align="center"}

::: {.footer}
Bakker, M., van Dijk, A., & Wicherts, J. M. (2012). The Rules of the Game Called Psychological Science. Perspectives on Psychological Science, 7(6), 543–554.
:::

##

![](img/graph12.png){fig-align="center"}

::: {.footer}
Fanelli, D. (2011). Negative results are disappearing from most disciplines and countries. Scientometrics, 90(3), 891–904. doi:10.1007/s11192-011-0494-7
:::


## Can we base our power analyses on published effect sizes?
**No.**

- See RP:P: 83% of all effect sizes are smaller than the original: <br> Mean original: r = .40 ➙ Mean replication: r = .20
- See also Franco et al. (2015): <br> Reported ES 2x larger than unreported ES

## Can we base our power analyses on published effect sizes?
• See Schäfer & Schwarz (2019), ES: r:

![](img/powerfromeffectsize.png){fig-align="center"}

::: {.footer}
Schäfer, T., & Schwarz, M. A. (2019). The meaningfulness of effect sizes in psychological research: Differences between sub-disciplines and the impact of potential biases.Frontiers in Psychology,10, 813.
:::

## Can we base our power analyses on published effect sizes?

- Suggestion 1: Divide reported effect by 2, compute power analysis.
- Suggestion 2: Safeguard power (Perugini, 2014): Incorporate uncertainty in original study’s ES estimate. Aim lower end of 60%-CI.

## Safeguard power 
(Perugini et al., 2014)

- Incorporate uncertainty in original study’s ES estimate
- Aim for lower end of 60%-CI
- Example: 
  - Original study finds d = 0.5 (n = 30 in each group)
  - 60% CI = [0.28; 0.72]
  - Naive 80% power analysis: n = 64
  - Safeguard 80% power analysis: n = 202
- Rewards precise estimates in original study
```{r, eval=FALSE,echo=TRUE}
library(MBESS)
ci.smd(smd=0.5, n.1=30, n.2=30, conf.level=0.60)
```

## Write-Up
![](img/writeup1.png){fig-align="center"}

::: {.footer}
[https://twitter.com/GuyProchilo/status/1292780240977223681](https://twitter.com/GuyProchilo/status/1292780240977223681)
:::

## Write-Up
![](img/writeup2.png){fig-align="center"}

::: {.footer}
Lakens (2021). Sample Size Justification. [https://psyarxiv.com/9d3yf/]( https://psyarxiv.com/9d3yf/)
:::

::: {.footer}
Perugini, M., Gallucci, M., & Costantini, G. (2014). Safeguard Power as a Protection Against Imprecise Power Estimates. Perspectives on Psychological Science, 9(3), 319–332. [http://doi.org/10.1177/1745691614528519](http://doi.org/10.1177/1745691614528519)
:::


::: footer
Siehe [https://www.fak11.lmu.de/dep_psychologie/studium/lehrelounge/benotung-schriftl-arbeiten/index.html](https://www.fak11.lmu.de/dep_psychologie/studium/lehrelounge/benotung-schriftl-arbeiten/index.html)
:::


<!--Footer insert below -->
```{r child="../../common/lastslide.qmd"}
```
